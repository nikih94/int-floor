{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO: Remove unnecessary imports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Conv1D, MaxPool2D, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from time import time\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import tensorflow as tf\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "from keijzer import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"} # Make sure the axis background of plots is white, this is usefull for the black theme in JupyterLab\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "# Setup (multi) GPU usage with scalable VRAM\n",
    "num_gpu = setup_multi_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator\n",
    "In this case it is not a Python generator, just a function that loads the data and outputs the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    # Loading the data\n",
    "    path = 'F:\\\\Jupyterlab\\\\Multivariate-time-series-models-in-Keras\\\\notebooks'\n",
    "    df = pd.read_csv(path+\"\\\\data\\\\house_data_processed.csv\", delimiter='\\t', parse_dates=['datetime'])\n",
    "    df = df.set_index(['datetime']) \n",
    "\n",
    "    magnitude = 1 # Take this from the 1. EDA & Feauture engineering notebook. It's the factor where gasPower has been scaled with to the power 10.\n",
    "    \n",
    "    # Preprocessing\n",
    "    data = df.copy()\n",
    "    \n",
    "    columns_to_category = ['hour', 'dayofweek', 'season']\n",
    "    data[columns_to_category] = data[columns_to_category].astype('category') # change datetypes to category\n",
    "    \n",
    "    # One hot encoding the dummy variables\n",
    "    data = pd.get_dummies(data, columns=columns_to_category) # One hot encoding the categories\n",
    "    \n",
    "    # Create train and test set\n",
    "    \n",
    "    X = data.drop(['gasPower'], axis=1)\n",
    "    #print('X columns: %s' % list(X.columns))\n",
    "\n",
    "    y = data['gasPower']\n",
    "\n",
    "    #X = np.array(X).reshape(-1,len(X.columns)) # Reshape to required dimensions for sklearn\n",
    "    #y = np.array(y).reshape(-1,1)\n",
    "\n",
    "    train_size = 0.7\n",
    "\n",
    "    split_index = int(data.shape[0]*train_size) # the index at which to split df into train and test\n",
    "\n",
    "    X_train = X[:split_index]\n",
    "    y_train = y[:split_index]\n",
    "\n",
    "    X_test = X[split_index:]\n",
    "    y_test = y[split_index:]\n",
    "    \n",
    "    # Scaling the features\n",
    "    scalerX = StandardScaler(with_mean=True, with_std=True).fit(X_train)\n",
    "\n",
    "    X_train = scalerX.transform(X_train)\n",
    "    X_test = scalerX.transform(X_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn_model(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Dense(16, input_shape=(X_train.shape[1],), kernel_initializer='TruncatedNormal'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout({{uniform(0, 1)}}, seed=seed)) \n",
    "\n",
    "    # If we choose 'four', add an additional fourth layer\n",
    "    if {{choice(['three', 'four'])}} == 'four':\n",
    "        model.add(Dense(100))\n",
    "\n",
    "        # We can also choose between complete sets of layers\n",
    "\n",
    "        model.add({{choice([Dropout(0.5), Activation('linear')])}})\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compiling the sequential model\n",
    "    model.compile(loss='mse', metrics=[mape, smape], optimizer={{choise(['rmsprop', 'adam', 'sgd', 'nadam'])}})\n",
    "    \n",
    "    result = model.fit(X_train, y_train, epochs=10, verbose=2, validation_split=0.3)\n",
    "    \n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_mape']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    \n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hyperas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e60fd07692a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhyperas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhyperas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m best_run, best_model = optim.minimize(model=create_model,\n\u001b[0;32m      5\u001b[0m                                           \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hyperas'"
     ]
    }
   ],
   "source": [
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials())\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the architecture\n",
    "model = load_model('models\\\\DNN.best.hdf5', custom_objects={'smape': smape, \n",
    "                                                    'mape': mape}) # Gave an error when loading without 'custom_objects'.. fixed by https://github.com/keras-team/keras/issues/3911\n",
    "\n",
    "# Compile with the same settings as it has been saved with earlier\n",
    "model.compile(loss='mse', metrics=[mape, smape], optimizer=adam)\n",
    "\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_true = y_test.values.reshape(y_test.shape[0], 1)\n",
    "\n",
    "split_index = int(data.shape[0]*train_size)\n",
    "x = data[split_index:]\n",
    "\n",
    "datetime_difference = len(x) - len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(x.index, y_true, '.-', color='red', label='Real values', alpha=0.5)\n",
    "plt.plot(x.index, y_pred, '.-', color='blue', label='Predicted values', alpha=1)\n",
    "\n",
    "plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % magnitude, fontsize=14)\n",
    "plt.xlabel('datetime [-]', fontsize=14) #TODO: set x values as actual dates\n",
    "\n",
    "plt.xticks(fontsize=14, rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.legend(loc='upper left', borderaxespad=0, frameon=False, fontsize=14, markerscale=3)\n",
    "\n",
    "mse_result, mape_result, smape_result = model.evaluate(X_test, y_test)\n",
    "\n",
    "plt.title('DNN result \\n MSE = %.2f \\n MAPE = %.1f [%%] \\n SMAPE = %.1f [%%]' % (mse_result, mape_result, smape_result), fontsize = 14)\n",
    "\n",
    "#plt.savefig('figures/Feedforward result hourly without dummy variables.png', dpi=1200)\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample these results to a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it a df to be able to downsample\n",
    "datetime = x.index\n",
    "print(datetime.shape)\n",
    "\n",
    "y_pred = y_pred.reshape(y_pred.shape[0])\n",
    "y_true = y_true.reshape(y_true.shape[0])\n",
    "\n",
    "results = pd.DataFrame(y_true, y_pred) # For some reason y_true becomes the index\n",
    "result = results.reset_index() # Ugly way to fix above problem\n",
    "result.columns = ['y_pred', 'y_true']\n",
    "\n",
    "result['datetime'] = datetime\n",
    "result = result.set_index(['datetime'])\n",
    "\n",
    "# Save the model results for later usage\n",
    "result.to_csv('models\\\\DNN_predictions.csv')\n",
    "\n",
    "result = result.resample('D').sum() # Resample data\n",
    "\n",
    "result = result.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics over the result\n",
    "\n",
    "ytrue = result['y_true']\n",
    "ypred = result['y_pred']\n",
    "n = len(result)\n",
    "\n",
    "# Recalculated the metrics for the downsampled results\n",
    "mse_result = (1/n)*np.sum((ypred - ytrue)**2)\n",
    "mape_result = (100/n) * np.sum(np.abs((ytrue - ypred) / ypred))\n",
    "smape_result = (100/n) * np.sum( np.abs((ytrue - ypred)) / (np.abs(ytrue) + np.abs(ypred)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(result.index, result['y_true'], '.-', color='red', label='Real values', alpha=0.5, ms=10) # ms is markersize\n",
    "plt.plot(result.index, result['y_pred'], '.-', color='blue', label='Predicted values', ms=10)\n",
    "\n",
    "plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % magnitude, fontsize=14)\n",
    "plt.xlabel('datetime [-]', fontsize=14) #TODO: set x values as actual dates\n",
    "\n",
    "plt.xticks(fontsize=14, rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.legend(loc='upper left', borderaxespad=0, frameon=False, fontsize=14, markerscale=3)\n",
    "\n",
    "plt.title('DNN result \\n MSE = %.2f \\n MAPE = %.1f [%%] \\n SMAPE = %.1f [%%]' % (mse_result, mape_result, smape_result), fontsize = 14)\n",
    "\n",
    "#plt.savefig('figures/LSTM result hourly resampled to daily by sum.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
